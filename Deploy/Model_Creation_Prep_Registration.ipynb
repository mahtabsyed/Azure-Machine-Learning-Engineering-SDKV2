{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6 Prep Model Creation & Registration - Model Deployment\n",
        "\n",
        "## In this notebook we will:\n",
        "\n",
        "  - Connect to your workspace.\n",
        "  - Create a virtual environment and leverage in this notebook\n",
        "  - Create Compute for running a job\n",
        "  - Create a job\n",
        "  - Configure your job\n",
        "  - Run the command\n",
        "  - Register the model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting yourself up for success\n",
        "\n",
        "- When creating a model, one of the major obstacles is having an environment that has the required dependencies.  We will create and register an AML environment and use on our compute instance.  This will allow us to leverage the model we build on a compute cluster on our compute instance.  The same packages and versions leveraged to build the model will be used to consume the model later in this notebook\n",
        "\n",
        "Steps to setup our environment include:\n",
        "- Connecting to our workspace\n",
        "- Defining and registering the environment\n",
        "- Making the environment available to our compute instance \n",
        "- Making the environment available to our jupyter notebook\n",
        "\n",
        "Let's get started\n",
        "\n",
        "Initially Select **Kernel** > **Change Kernel** > **Python 3.10 - SDK V2**\n",
        "\n",
        "or if you already setup the virtual environment in Chapter 4:\n",
        "\n",
        "Select **Kernel** > **Change Kernel** > **job_env**\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "import pandas as pd\n",
        "import time\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml.entities import Environment, BuildContext"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1719740099621
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to your workspace\n",
        "\n",
        "Once you connect to your workspace, you will create a new cpu target which you will provide an environment to.\n",
        "\n",
        "- Configure your credential.  We are using `DefaultAzureCredential`.  It will request a token using multiple identities, stopping once a token is found"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get a handle to the workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential(), path=\"../Azure-Machine-Learning-Engineering-SDKV2/\")\n",
        "ml_client"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7fd57be00fd0>,\n         subscription_id=d9304a62-bdd8-46df-bf0c-107ccc0e8d12,\n         resource_group_name=amls-dev-rg,\n         workspace_name=amls-dev)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1719740113087
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup enviroment\n",
        "\n",
        "### Creating environment from docker image with a conda YAML\n",
        "\n",
        "Azure ML allows you to leverage curated environments, as well as to build your own environment from:\n",
        "\n",
        "    - existing docker image\n",
        "    - base docker image with a conda yml file to customize\n",
        "    - a docker build content\n",
        "    \n",
        "We will proceed with creating an environment from a docker build plus a conda yml file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "script_folder = os.path.join(os.getcwd(), \"conda-yamls\")\n",
        "print(script_folder)\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/amldevcomputev2/code/Users/270351/Azure-Machine-Learning-Engineering-SDKV2/Deploy/conda-yamls\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1719740153366
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create job environment in a yml file"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The yml file below can be used to create the conda environment for running this notebook provided the kernel `job_env` is not currently available for you"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda-yamls/job_env.yml\n",
        "name: job_env\n",
        "dependencies:\n",
        "- python=3.10\n",
        "- scikit-learn=1.1.3\n",
        "- ipykernel\n",
        "- matplotlib\n",
        "- pandas\n",
        "- pip\n",
        "- pip:\n",
        "  - mlflow==2.0.1\n",
        "  - azure-ai-ml==1.1.2\n",
        "  - mltable==1.0.0\n",
        "  - azureml-mlflow==1.48.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting conda-yamls/job_env.yml\n"
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The yml file below will be used for creating your model.  It is nearly the same as the job_env, but given our model will not be leveraging `mltable` we have excluded it from the model build environment."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda-yamls/job_env_for_build.yml\n",
        "name: job_env\n",
        "dependencies:\n",
        "- python=3.10\n",
        "- scikit-learn=1.1.3\n",
        "- ipykernel\n",
        "- matplotlib\n",
        "- pandas\n",
        "- pip\n",
        "- pip:\n",
        "  - mlflow==2.0.1\n",
        "  - azure-ai-ml==1.1.2\n",
        "  - azureml-mlflow==1.48.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting conda-yamls/job_env_for_build.yml\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use your virtual environment in this notebook\n",
        "\n",
        "If you do not already have the virtual environment `job_env` available when you go to `Kernel` -> `Change Kernel`, you can follow the instructions below to upate your virtual enviornment available to your jupyter notebook.  If you created the virtual environment in **Chapter 4**, then you can use it now, else follow the instructions below to create the environment.\n",
        "\n",
        "\n",
        "We can actually use that virtual environment on our compute instance and in this very jupyter notebook.\n",
        "Open a terminal session, and cd into your conda-yamls folder and run the following commands:\n",
        "\n",
        "```\n",
        "cd Azure-Machine-Learning-Engineering/\n",
        "cd Chapter06\n",
        "cd conda-yamls/\n",
        "conda env create -f job_env.yml\n",
        "conda activate job_env\n",
        "ipython kernel install --user --name job_env --display-name \"job_env\"\n",
        "```\n",
        "* After the environment has been made available to Jupyter, Refresh this session (F5, or Hit refresh on your browser)\n",
        "\n",
        "When you go to your `Kernel` -> `Change Kernel`, it will be available to select.  You will have to rerun the notebook from the beginning, but when you download the model, you will be using all of the correct versions of libraries.\n",
        "\n",
        "If you run the next cell, and you get an error message, `No module named 'sklearn'` that means that you did not setup the conda virtual environment acess mentioned here."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import mlflow\n",
        "import azure.ai.ml\n",
        "print ('sklearn: {}'. format (sklearn. __version__))\n",
        "print('azure.ai.ml: {}'.format(azure.ai.ml._version.VERSION))\n",
        "\n",
        "print(\"This notebook was created using sklearn: 1.1.3\")\n",
        "print(\"This notebook was created using azure.ai.ml: 1.1.2\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sklearn: 1.1.3\nazure.ai.ml: 1.1.2\nThis notebook was created using sklearn: 1.1.3\nThis notebook was created using azure.ai.ml: 1.1.2\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1719740164046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the most current and up-to-date base image\n",
        "\n",
        "Default images are always changing.  \n",
        "Note the base image is defined in the property `image` below.  These images are defined at [https://hub.docker.com/_/microsoft-azureml](https://hub.docker.com/_/microsoft-azureml)\n",
        "\n",
        "The current image we have selected for this notebook is `mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04`, but based on image availability, that will change in the future.  In additon, note the python version specified in your conda environment file is `python=3.10`, as this will evolve over time as well. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
        "    conda_file=\"conda-yamls/job_env_for_build.yml\",\n",
        "    name=\"job_base_for_build_env\",\n",
        "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
        ")\n",
        "env = ml_client.environments.create_or_update(env_docker_conda)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1719740178214
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.name)\n",
        "print(env.version)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "job_base_for_build_env\n1\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1719740196185
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous chapter, you registered a dataset, if you have not already registered the dataset, it has beeen added to this chapter and will be registered below"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "try:\n",
        "    registered_data_asset = ml_client.data.get(name='titanic_prepped', version=1)\n",
        "    print('data asset is registered')\n",
        "except:\n",
        "    print('register data asset')\n",
        "    my_data = Data(\n",
        "        path=\"./prepped_data/titanic_prepped.csv\",\n",
        "        type=AssetTypes.URI_FILE,\n",
        "        description=\"Titanic CSV\",\n",
        "        name=\"titanic_prepped\",\n",
        "        version=\"1\",\n",
        "    )\n",
        "\n",
        "    ml_client.data.create_or_update(my_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "register data asset\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading titanic_prepped.csv\u001b[32m (< 1 MB): 100%|██████████| 20.6k/20.6k [00:00<00:00, 866kB/s]\n\u001b[39m\n\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1719740206600
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Compute "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# specify aml compute name.\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    ml_client.compute.get(cpu_compute_target)\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "    compute = AmlCompute(\n",
        "        name=cpu_compute_target, size=\"STANDARD_D2_V2\", min_instances=0, max_instances=4, idle_time_before_scale_down = 3600\n",
        "    )\n",
        "    ml_client.compute.begin_create_or_update(compute)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1719740250320
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating code to generate Basic Model\n",
        "\n",
        "We will first create a model using the job command"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "script_folder = os.path.join(os.getcwd(), \"src\")\n",
        "print(script_folder)\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/amldevcomputev2/code/Users/270351/Azure-Machine-Learning-Engineering-SDKV2/Deploy/src\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1719740256657
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create main.py file for running in your command"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./src/main.py\n",
        "import os\n",
        "import argparse\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.models import infer_signature\n",
        "from mlflow.utils.environment import _mlflow_conda_env\n",
        "from mlflow.tracking import MlflowClient\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score,roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "# define functions\n",
        "def main(args):\n",
        "    # enable auto logging\n",
        "    current_run = mlflow.start_run()\n",
        "    mlflow.sklearn.autolog(log_models=False)\n",
        "\n",
        "    # read in data\n",
        "    df = pd.read_csv(args.titanic_csv)\n",
        "    model = model_train('Survived', df, args.randomstate)\n",
        "    mlflow.end_run()\n",
        "\n",
        "def model_train(LABEL, df, randomstate):\n",
        "    print('df.columns = ')\n",
        "    print(df.columns)\n",
        "    \n",
        "    df['Embarked'] = df['Embarked'].astype(object)\n",
        "    df['Loc'] = df['Loc'].astype(object)\n",
        "    df['Loc'] = df['Sex'].astype(object)\n",
        "    df['Pclass'] = df['Pclass'].astype(float)\n",
        "    df['Age'] = df['Age'].astype(float)\n",
        "    df['Fare'] = df['Fare'].astype(float)\n",
        "    df['GroupSize'] = df['GroupSize'].astype(float)\n",
        "\n",
        "    y_raw           = df[LABEL]\n",
        "    columns_to_keep = ['Embarked', 'Loc', 'Sex','Pclass', 'Age', 'Fare', 'GroupSize']\n",
        "    X_raw           = df[columns_to_keep]\n",
        "\n",
        "    print(X_raw.columns)\n",
        "     # Train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=randomstate)\n",
        "    \n",
        "    #use Logistic Regression estimator from scikit learn\n",
        "    lg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
        "    preprocessor = buildpreprocessorpipeline(X_train)\n",
        "    \n",
        "    #estimator instance\n",
        "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('regressor', lg)], verbose=True)\n",
        "\n",
        "    model = clf.fit(X_train, y_train)\n",
        "    \n",
        "    print('type of X_test = ' + str(type(X_test)))\n",
        "          \n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print('*****X_test************')\n",
        "    print(X_test)\n",
        "    \n",
        "    #get the active run.\n",
        "    run = mlflow.active_run()\n",
        "    print(\"Active run_id: {}\".format(run.info.run_id))\n",
        "\n",
        "    acc = model.score(X_test, y_test )\n",
        "    print('Accuracy:', acc)\n",
        "    MlflowClient().log_metric(run.info.run_id, \"test_acc\", acc)\n",
        "    \n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' , auc)\n",
        "    MlflowClient().log_metric(run.info.run_id, \"test_auc\", auc)\n",
        "    \n",
        "    \n",
        "    # Signature\n",
        "    signature = infer_signature(X_test, y_test)\n",
        "\n",
        "    # Conda environment\n",
        "    custom_env =_mlflow_conda_env(\n",
        "        additional_conda_deps=[\"scikit-learn==1.1.3\"],\n",
        "        additional_pip_deps=[\"mlflow<=1.30.0\"],\n",
        "        additional_conda_channels=None,\n",
        "    )\n",
        "\n",
        "    # Sample\n",
        "    input_example = X_train.sample(n=1)\n",
        "\n",
        "    # Log the model manually\n",
        "    mlflow.sklearn.log_model(model, \n",
        "                             artifact_path=\"model\", \n",
        "                             conda_env=custom_env,\n",
        "                             signature=signature,\n",
        "                             input_example=input_example)\n",
        "\n",
        "\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def buildpreprocessorpipeline(X_raw):\n",
        "\n",
        "    categorical_features = X_raw.select_dtypes(include=['object', 'bool']).columns\n",
        "    numeric_features = X_raw.select_dtypes(include=['float','int64']).columns\n",
        "\n",
        "    #categorical_features = ['Sex', 'Embarked', 'Loc']\n",
        "    categorical_transformer = Pipeline(steps=[('onehotencoder', \n",
        "                                               OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'))])\n",
        "\n",
        "\n",
        "    #numeric_features = ['Pclass', 'Age', 'Fare', 'GroupSize']    \n",
        "    numeric_transformer1 = Pipeline(steps=[('scaler1', SimpleImputer(missing_values=np.nan, strategy = 'mean'))])\n",
        "    \n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('numeric1', numeric_transformer1, numeric_features),\n",
        "            ('categorical', categorical_transformer, categorical_features)], remainder='drop')\n",
        "    \n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--titanic-csv\", type=str)\n",
        "    parser.add_argument(\"--randomstate\", type=int, default=42)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./src/main.py\n"
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Command\n",
        "\n",
        "- `display_name` display name for the job\n",
        "- `description`  the description of the experiment\n",
        "- `code` path where the code is located\n",
        "- `command` command to run\n",
        "- `inputs`  dictionary of name value pairs using `${{inputs.<input_name>}}`\n",
        "    \n",
        "    - To use files or folder - using the `Input` class\n",
        "        \n",
        "        - `type` defaults to a `uri_folder` but this can be set to `uri_file` or `uri_folder`\n",
        "        - `path` is the path to the file or folder.  These can be local or remote leveraging **https, http, wasb`\n",
        "        \n",
        "            - To use an Azure ML dataset, this would be an Input `Input(type='uri_folder', path='my_dataset:1')`\n",
        "            \n",
        "            - `mode` is how the data should be delivered to the compute which include `ro_mount`(default), `rw_mount` and `download`\n",
        "\n",
        "- `environment`: environment to be used by compute when running command\n",
        "- `compute`: can be `local`, or a specificed compute name\n",
        "- `distribution`: distribution to leverage for distributed training scenerios including:\n",
        "        \n",
        "    - `Pytorch`\n",
        "    - `TensorFlow`\n",
        "    - `MPI`\n",
        "            "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create the command\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input\n",
        "\n",
        "my_job = command(\n",
        "    code=\"./src\",  # local path where the code is stored\n",
        "    command=\"python main.py --titanic ${{inputs.titanic}} --randomstate ${{inputs.randomstate}}\",\n",
        "    inputs={\n",
        "        \"titanic\": Input(\n",
        "            type=\"uri_file\",\n",
        "            path=\"azureml:titanic_prepped:1\",\n",
        "        ),\n",
        "        \"randomstate\": 0,\n",
        "    },\n",
        "    environment=\"job_base_for_build_env@latest\",\n",
        "    compute=\"cpu-cluster\",\n",
        "    display_name=\"sklearn-titanic\",\n",
        "    # description,\n",
        "    # experiment_name\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1719740343792
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_folder = os.path.join(os.getcwd(), \"job\")\n",
        "print(script_folder)\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/amldevcomputev2/code/Users/270351/Azure-Machine-Learning-Engineering-SDKV2/Deploy/job\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1719740372130
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Command with SDK"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the command\n",
        "returned_job = ml_client.create_or_update(my_job)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading src (0.0 MBs):   0%|          | 0/4936 [00:00<?, ?it/s]\r\u001b[32mUploading src (0.0 MBs): 100%|██████████| 4936/4936 [00:00<00:00, 77723.03it/s]\n\u001b[39m\n\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1719740379967
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register the Model \n",
        "\n",
        "Using the Python SDK V2 - we can register the Model for use.  \n",
        "\n",
        "Parameters for model registration include:\n",
        "\n",
        "- `path` - A remote uri or local path pointing at the model\n",
        "- `name` - A string value\n",
        "- `description` - A description for the model\n",
        "- `type` - valid values include: \n",
        "    - \"custom_model\"\n",
        "    - \"mlflow_model\" \n",
        "    - \"triton_model\".  \n",
        "    \n",
        "* Instead of typing out the `type`, you can use the AssetTypes in the namespace azure.ai.ml.constants as we have done below\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run_id = returned_job.name\n",
        "print('runid:' + run_id)\n",
        "experiment = returned_job.experiment_name\n",
        "print(\"experiment:\" + experiment)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "runid:dynamic_van_jcccw11s7r\nexperiment:Deploy\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1719740495531
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking on job status\n",
        "\n",
        "When the job is created, the image will be prepared, and pushed to your Azure Container Registry.  If the compute cluster is down, it will also be spun up, the image will be loaded onto the compute cluster, and the job will be started.  Initially, this image does not exist, so you will see that the first time you submit your job, it will take some time to complete, but future runs will be able to re-use this image and will start up right away provided your compute cluster is up"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "exp = mlflow.get_experiment_by_name(experiment)\n",
        "last_run = mlflow.search_runs(exp.experiment_id, output_format=\"list\")[-1]\n",
        "\n",
        "if last_run.info.run_id != run_id:\n",
        "    print('run ids were not the same - waiting for run id to update')\n",
        "    time.sleep(5)\n",
        "    exp = mlflow.get_experiment_by_name(experiment)\n",
        "    last_run = mlflow.search_runs(exp.experiment_id, output_format=\"list\")[-1]\n",
        "\n",
        "while last_run.info.status == 'SCHEDULED':\n",
        "  print('run is being scheduled')\n",
        "  time.sleep(15)\n",
        "  last_run = mlflow.search_runs(exp.experiment_id, output_format=\"list\")[-1]\n",
        "\n",
        "while last_run.info.status == 'RUNNING':\n",
        "  print('job is being run')\n",
        "  time.sleep(15)\n",
        "  last_run = mlflow.search_runs(exp.experiment_id, output_format=\"list\")[-1]\n",
        "\n",
        "print(\"run_id:{}\".format(last_run.info.run_id))\n",
        "print('----------')\n",
        "print(\"run_id:{}\".format(last_run.info.status))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "run is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\nrun is being scheduled\njob is being run\njob is being run\njob is being run\njob is being run\njob is being run\njob is being run\njob is being run\njob is being run\njob is being run\njob is being run\njob is being run\nrun_id:dynamic_van_jcccw11s7r\n----------\nrun_id:FINISHED\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1719741228111
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the Model\n",
        "\n",
        "In the next notebook we will get the model directly from the run, but you can register a model from a run as shown below, and review the model in your AMLS workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.constants import ModelType\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "run_model = Model(\n",
        "    path=\"azureml://jobs/\" + last_run.info.run_id  + \"/outputs/artifacts/paths/model/\",\n",
        "    name=\"chapter6_titanic_model\",\n",
        "    description=\"Model created from run.\",\n",
        "    type=AssetTypes.MLFLOW_MODEL\n",
        ")\n",
        "\n",
        "ml_client.models.create_or_update(run_model) "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "Model({'job_name': 'dynamic_van_jcccw11s7r', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'chapter6_titanic_model', 'description': 'Model created from run.', 'tags': {}, 'properties': {}, 'id': '/subscriptions/d9304a62-bdd8-46df-bf0c-107ccc0e8d12/resourceGroups/amls-dev-rg/providers/Microsoft.MachineLearningServices/workspaces/amls-dev/models/chapter6_titanic_model/versions/1', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/amldevcomputev2/code/Users/270351/Azure-Machine-Learning-Engineering-SDKV2/Deploy', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fd576dee830>, 'serialize': <msrest.serialization.Serializer object at 0x7fd576def5e0>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/d9304a62-bdd8-46df-bf0c-107ccc0e8d12/resourceGroups/amls-dev-rg/workspaces/amls-dev/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.dynamic_van_jcccw11s7r/model', 'datastore': None, 'utc_time_created': None, 'flavors': {'python_function': {'env': '{\\n  \"conda\": \"conda.yaml\",\\n  \"virtualenv\": \"python_env.yaml\"\\n}', 'loader_module': 'mlflow.sklearn', 'model_path': 'model.pkl', 'predict_fn': 'predict', 'python_version': '3.10.14'}, 'sklearn': {'code': '', 'pickled_model': 'model.pkl', 'serialization_format': 'cloudpickle', 'sklearn_version': '1.1.3'}}, 'arm_type': 'model_version', 'type': 'mlflow_model'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1719741678299
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_model"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "Model({'job_name': None, 'is_anonymous': False, 'auto_increment_version': True, 'name': 'chapter6_titanic_model', 'description': 'Model created from run.', 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/amldevcomputev2/code/Users/270351/Azure-Machine-Learning-Engineering-SDKV2/Deploy', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fd576dee2c0>, 'version': None, 'latest_version': None, 'path': 'azureml://jobs/dynamic_van_jcccw11s7r/outputs/artifacts/paths/model/', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'mlflow_model'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1719741706523
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_model.path"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "'azureml://jobs/dynamic_van_jcccw11s7r/outputs/artifacts/paths/model/'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1719741710328
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "job_env"
    },
    "kernelspec": {
      "name": "job_env",
      "language": "python",
      "display_name": "job_env"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}